1. clone the git repo
  git clone https://github.com/chocjy/randomized-LS-solvers.git -b exp

2. get settings_jiyan.cfg

[directories]
data_dir = ../data/
hdfs_dir = data/
spark_logs_dir = /../log/

[s3]
key_id = AKIAI2R46NYDXGT2LJEQ
secret_key = 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

3. fetch x_opt and f_opt files

Here is a dumb way to do step 3:
export AWS_ACCESS_KEY_ID=AKIAI2R46NYDXGT2LJEQ
export AWS_SECRET_ACCESS_KEY=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

hadoop fs -copyToLocal s3n://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@jiyan/rand_matrix_alg_data/nonunif_bad_250000_1000_x_opt.txt .
hadoop fs -copyToLocal s3n://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@jiyan/rand_matrix_alg_data/nonunif_bad_250000_1000_f_opt.txt .

export DRIVER_MACHINE_NAME=ec2-52-25-32-139.us-west-2.compute.amazonaws.com
scp -i test.pem nonunif_bad_250000_1000_*_opt.txt root@${DRIVER_MACHINE_NAME}:~/randomized-LS-solvers/data/

4. add spark bin dir to path
  PATH=$PATH:~/spark/bin/

5. modify the submit command to load data from S3
  (a) use --s3
  (b) increase the memory requested

